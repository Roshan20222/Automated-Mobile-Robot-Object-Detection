<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Object Detection Based Automated Mobile Robot - Research by Roshan Pandey">
    <meta name="author" content="Roshan Pandey">
    <title>Object Detection Based Automated Mobile Robot | Roshan Pandey</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="container">
            <div class="nav-brand">
                <a href="index.html">Roshan Pandey</a>
            </div>
            <ul class="nav-menu">
                <li><a href="#about">About</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="hero">
        <div class="container">
            <h1 class="project-title">Object Detection Based Automated Mobile Robot</h1>
            <p class="project-subtitle">Autonomous pick-and-place system using YOLO v8 and robotic arm kinematics</p>
            <div class="project-meta">
                <span class="author">Roshan Pandey</span> | 
                <span class="institution">Tribhuvan University, Institute of Engineering</span> | 
                <span class="date">September 2024</span>
            </div>
            <div class="project-links">
                <a href="papers/major-project-report.pdf" class="btn btn-primary" target="_blank">
                    üìÑ Full Report (PDF)
                </a>
                <a href="https://github.com/Roshan20222/Automated-Mobile-Robot-Object-Detection" class="btn btn-secondary" target="_blank">
                    üíª GitHub Repository
                </a>
            </div>
        </div>
    </section>

    <!-- Abstract Section -->
    <section class="section">
        <div class="container">
            <h2>Abstract</h2>
            <p class="abstract">
                This project proposes the control design and implementation of an object detection based 
                automated mobile robot integrated with a robotic arm. The system leverages computer vision 
                algorithms (YOLO v8) to analyze visual information captured by cameras in real-time. 
                The images are processed to extract relevant features such as object detection, tracking, 
                and pose estimation, which generate control signals for precise manipulation of the mobile robot. 
                Our system integrates state-of-the-art techniques in computer vision, machine learning, 
                trajectory optimization, and visual servoing.
            </p>
            <div class="keywords">
                <strong>Keywords:</strong> Robotics, Machine Learning, Automation, YOLO, Computer Vision, 
                Kinematics, Object Detection
            </div>
        </div>
    </section>

    <!-- System Overview with Image -->
    <section class="section bg-light">
        <div class="container">
            <h2>System Architecture</h2>
            <div class="content-grid">
                <div class="content-text">
                    <h3>Key Components</h3>
                    <ul>
                        <li><strong>Processing Unit:</strong> Raspberry Pi 4 (Quad-core 1.2GHz, 4GB RAM)</li>
                        <li><strong>Vision System:</strong> Raspberry Pi Camera (5MP) + USB Webcam</li>
                        <li><strong>Mobile Platform:</strong> 4 DC Motors with L298N Motor Driver</li>
                        <li><strong>Manipulation:</strong> 4-DOF Robotic Arm with MG90S Servo Motors</li>
                        <li><strong>Sensors:</strong> HC-SR04 Ultrasonic Sensor for distance measurement</li>
                        <li><strong>Power:</strong> LiPo Battery with 5V Buck Converter</li>
                    </ul>
                </div>
                <div class="content-image">
                    <img src="images/major-project/Block Diagram of Proposed System.png" 
                         alt="Block Diagram of Proposed System" 
                         class="responsive-img">
                    <p class="image-caption">Figure 1: Block diagram showing system architecture</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Hardware Integration -->
    <section class="section">
        <div class="container">
            <h2>Hardware Integration</h2>
            <div class="image-gallery">
                <div class="gallery-item">
                    <img src="images/major-project/Raspberry Pi.png" alt="Raspberry Pi" class="responsive-img">
                    <p class="image-caption">Raspberry Pi 4 - Central Processing Unit</p>
                </div>
                <div class="gallery-item">
                    <img src="images/major-project/Hardware Assembled.png" alt="Hardware Assembly" class="responsive-img">
                    <p class="image-caption">Complete Hardware Assembly</p>
                </div>
                <div class="gallery-item">
                    <img src="images/major-project/Coordinate Frame of 4 DOF Arm Robot.png" 
                         alt="4-DOF Arm Coordinate Frame" class="responsive-img">
                    <p class="image-caption">4-DOF Robotic Arm Coordinate Frame</p>
                </div>
            </div>
            <p>
                The hardware integration was completed in two phases. First, the mobile platform was assembled 
                with four DC motors connected to wheels, controlled through an L298N motor driver. The Raspberry Pi 
                coordinates all components through GPIO interfaces. Second, a 4-DOF robotic arm was designed using 
                CAD software and fabricated via 3D printing (print speed: 80mm/s, hot-end temp: 200¬∞C, 
                bed temp: 60¬∞C, layer height: 0.2mm, infill: 20%).
            </p>
        </div>
    </section>

    <!-- Methodology -->
    <section class="section bg-light">
        <div class="container">
            <h2>Methodology</h2>
            
            <h3>Object Detection Using YOLO v8</h3>
            <div class="content-grid">
                <div class="content-text">
                    <p>
                        The system employs YOLO (You Only Look Once) v8 for real-time object detection. 
                        The model was trained on a custom dataset containing red boxes, yellow boxes, 
                        robot images, and destination markers.
                    </p>
                    <h4>Data Augmentation</h4>
                    <ul>
                        <li>Rotation and flipping transformations</li>
                        <li>Zoom and crop operations</li>
                        <li>Brightness and contrast adjustments</li>
                        <li>Noise addition for robustness</li>
                    </ul>
                    <p>
                        Dataset split: 70% training, 20% validation, 10% testing. 
                        Images were labeled using CVAT (Computer Vision Annotation Tool). 
                        The model was trained for 50 epochs using the Ultralytics package.
                    </p>
                </div>
                <div class="content-image">
                    <img src="images/major-project/YOLO Architecture.png" 
                         alt="YOLO Architecture" class="responsive-img">
                    <p class="image-caption">Figure 2: YOLO Neural Network Architecture</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section class="section">
        <div class="container">
            <h2>Experimental Results</h2>
            
            <h3>Object Detection Performance</h3>
            <div class="results-grid">
                <div class="result-item">
                    <img src="images/major-project/Performance Matrix Graphs.png" 
                         alt="Performance Metrics" class="responsive-img">
                    <p class="image-caption">Training loss and accuracy curves over 50 epochs</p>
                </div>
                <div class="result-item">
                    <img src="images/major-project/Confusion Matrix.png" 
                         alt="Confusion Matrix" class="responsive-img">
                    <p class="image-caption">Confusion matrix showing classification accuracy</p>
                </div>
            </div>

            <div class="metrics-table">
                <h4>Performance Metrics</h4>
                <table>
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Precision</td>
                            <td>92%</td>
                        </tr>
                        <tr>
                            <td>Recall</td>
                            <td>89%</td>
                        </tr>
                        <tr>
                            <td>Object Detection Success Rate</td>
                            <td>94%</td>
                        </tr>
                        <tr>
                            <td>Grasping Success Rate</td>
                            <td>91%</td>
                        </tr>
                        <tr>
                            <td>Inference Time</td>
                            <td>45 ms/frame</td>
                        </tr>
                        <tr>
                            <td>Positioning RMSE</td>
                            <td>2.3 mm</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Detection and Path Planning</h3>
            <div class="results-grid">
                <div class="result-item">
                    <img src="images/major-project/Labeled Image Data.png" 
                         alt="Labeled Training Data" class="responsive-img">
                    <p class="image-caption">Labeled training images with bounding boxes</p>
                </div>
                <div class="result-item">
                    <img src="images/major-project/Predicted Image.png" 
                         alt="Prediction Results" class="responsive-img">
                    <p class="image-caption">Real-time detection with confidence scores</p>
                </div>
            </div>

            <div class="results-grid">
                <div class="result-item">
                    <img src="images/major-project/Predicted image with grid and coordinates.png" 
                         alt="Grid System" class="responsive-img">
                    <p class="image-caption">Grid-based coordinate system for path planning</p>
                </div>
                <div class="result-item">
                    <img src="images/major-project/Predicted Image with coordinates and path.png" 
                         alt="Path Planning" class="responsive-img">
                    <p class="image-caption">Dijkstra's algorithm path from robot to target</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Kinematics Section -->
    <section class="section bg-light">
        <div class="container">
            <h2>Robotic Arm Kinematics</h2>
            
            <h3>Forward and Inverse Kinematics</h3>
            <p>
                The 4-DOF robotic arm employs Denavit-Hartenberg (D-H) parameters for forward kinematics 
                calculation and algebraic approach for inverse kinematics. This enables precise end-effector 
                positioning for object manipulation.
            </p>

            <div class="equation-box">
                <h4>Forward Kinematics End-Effector Position:</h4>
                <div class="equation">
                    <p><em>P<sub>x</sub></em> = cos(Œ∏‚ÇÅ)[130cos(Œ∏‚ÇÇ + Œ∏‚ÇÉ + Œ∏‚ÇÑ) + 216cos(Œ∏‚ÇÇ + Œ∏‚ÇÉ) + 180cos(Œ∏‚ÇÇ) + 100cos(Œ∏‚ÇÇ)]</p>
                    <p><em>P<sub>y</sub></em> = sin(Œ∏‚ÇÅ)[130cos(Œ∏‚ÇÇ + Œ∏‚ÇÉ + Œ∏‚ÇÑ) + 216cos(Œ∏‚ÇÇ + Œ∏‚ÇÉ) + 180cos(Œ∏‚ÇÇ) + 100cos(Œ∏‚ÇÇ)]</p>
                    <p><em>P<sub>z</sub></em> = 145 - 108sin(Œ∏‚ÇÇ + Œ∏‚ÇÉ) - 90sin(Œ∏‚ÇÇ) - 130cos(Œ∏‚ÇÇ + Œ∏‚ÇÉ + Œ∏‚ÇÑ)</p>
                </div>
            </div>

            <div class="content-grid">
                <div class="content-text">
                    <h4>Inverse Kinematics Approach:</h4>
                    <ol>
                        <li>Calculate base rotation: Œ∏‚ÇÅ = atan2(P<sub>y</sub>, P<sub>x</sub>)</li>
                        <li>Compute radial distance and solve for joint angles Œ∏‚ÇÇ, Œ∏‚ÇÉ, Œ∏‚ÇÑ</li>
                        <li>Apply geometric constraints and joint limits</li>
                        <li>Validate solution with forward kinematics</li>
                    </ol>
                </div>
                <div class="content-image">
                    <img src="images/major-project/Flow Chart for motion planning.png" 
                         alt="Motion Planning Flowchart" class="responsive-img">
                    <p class="image-caption">Motion planning algorithm flowchart</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Applications Section -->
    <section class="section">
        <div class="container">
            <h2>Applications</h2>
            <div class="applications-grid">
                <div class="app-card">
                    <h3>üè≠ Manufacturing</h3>
                    <p>Product assembly, parts welding, surface painting, and material handling in plants</p>
                </div>
                <div class="app-card">
                    <h3>üì¶ Logistics</h3>
                    <p>Cargo loading/unloading from trucks, ships, and airplanes; warehouse operations</p>
                </div>
                <div class="app-card">
                    <h3>üèóÔ∏è Construction</h3>
                    <p>Building inspection and repair, infrastructure maintenance, material transport</p>
                </div>
                <div class="app-card">
                    <h3>üéñÔ∏è Military</h3>
                    <p>Weapons handling, equipment inspection/repair, bomb disposal operations</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Publications Section -->
    <section class="section bg-light" id="publications">
        <div class="container">
            <h2>Publications & Documentation</h2>
            <div class="publication-item">
                <h3>Major Project Report: Object Detection Based Automated Mobile Robot</h3>
                <p class="pub-meta">
                    <strong>Author:</strong> Roshan Pandey<br>
                    <strong>Institution:</strong> Kathmandu Engineering College, Tribhuvan University<br>
                    <strong>Department:</strong> Electronics, Communication and Information Engineering<br>
                    <strong>Date:</strong> September 2024
                </p>
                <p class="pub-abstract">
                    Complete major project report detailing the design, implementation, hardware integration, 
                    software algorithms, experimental results, and analysis of the automated mobile robot system.
                </p>
                <div class="pub-links">
                    <a href="papers/major-project-report.pdf" class="btn btn-primary" target="_blank">Download PDF</a>
                    <a href="https://github.com/Roshan20222/Automated-Mobile-Robot-Object-Detection" class="btn btn-secondary" target="_blank">View Code</a>
                </div>
            </div>

            <div class="publication-item">
                <h3>Research Paper: Object Detection Based Automated Mobile Robot With Robotic Arm</h3>
                <p class="pub-meta">
                    <strong>Author:</strong> Roshan Pandey<br>
                    <strong>Institution:</strong> Kathmandu Engineering College, Tribhuvan University<br>
                    <strong>Date:</strong> September 2025
                </p>
                <p class="pub-abstract">
                    Peer-reviewed research paper presenting the design, control, and implementation of 
                    object detection-based automated mobile robot with performance metrics and validation results.
                </p>
                <div class="pub-links">
                    <a href="papers/research-paper.pdf" class="btn btn-primary" target="_blank">Download PDF</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Contact Section -->
    <section class="section" id="contact">
        <div class="container">
            <h2>Contact</h2>
            <div class="contact-info">
                <p><strong>Roshan Pandey</strong></p>
                <p>Ph.D. Student in Computer Science</p>
                <p>University of California, Berkeley (Expected 2029)</p>
                <p>Email: <a href="mailto:pandeyroshan20222@gmail.com">pandeyroshan20222@gmail.com</a></p>
                <p>GitHub: <a href="https://github.com/Roshan20222" target="_blank">github.com/Roshan20222</a></p>
                <p>Portfolio: <a href="https://roshan20222.github.io" target="_blank">roshan20222.github.io</a></p>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2024 Roshan Pandey. All rights reserved.</p>
        </div>
    </footer>

    <script src="js/main.js"></script>
</body>
</html>
